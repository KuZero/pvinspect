{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitpvinspect37condaedaa7dc3546640f79a12e9df0cc33085",
   "display_name": "Python 3.7.7 64-bit ('pvinspect37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch (Lightning) integration\n",
    "\n",
    "This package includes an integration with PyTorch that allows you to convert an `ImageSequence` into a PyTorch `Dataset` in a single line of code. This can then be used to train models using PyTorch and derived frameworks, such als [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning). This notebook showcases the use and walks you through the whole process from loading data to training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvinspect as pv\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "from typing import List\n",
    "from sklearn import model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up `LightningModule`\n",
    "\n",
    "We set up a very basic `LightningModule` for classification of defects on solar cells. We omit the dataloaders, since we pass them in dynamically lateron. For more information, please refer to the [docs](https://pytorch-lightning.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_weight: List[float],\n",
    "        learning_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # let's use a very small resnet\n",
    "        self.model = tv.models.ResNet(tv.models.resnet.BasicBlock, layers=[1, 1, 1, 1], num_classes=2)\n",
    "\n",
    "        self.pos_weight = t.tensor(pos_weight)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'loss': t.nn.functional.binary_cross_entropy_with_logits(y_hat, y, pos_weight=self.pos_weight.to(x))}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'loss': t.nn.functional.binary_cross_entropy_with_logits(y_hat, y, pos_weight=self.pos_weight.to(x))}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return t.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load data\n",
    "\n",
    "Here, we'll use the cell images provided with the [ELPV dataset](https://github.com/zae-bayern/elpv-dataset) [[1](http://dx.doi.org/10.1016/j.solener.2019.02.067)]. This toolbox provides a convenience method for loading this data as well as additional defect annotations. In addition, a split in training and test data is provided by the meta property `testset`. Here, we'll only use the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2624/2624 [00:04<00:00, 574.74it/s]\n"
    }
   ],
   "source": [
    "all_data = pv.data.datasets.elpv().pandas.query('testset == False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train/validation split\n",
    "\n",
    "We'll train for two of the defect classes (`crack` and `inactive`). To this end, we map labels into the label powerset and use this to perform a stratified 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate label powerset\n",
    "is_crack = np.array(all_data.meta_to_pandas()['crack'].to_list())*1\n",
    "is_inactive = np.array(all_data.meta_to_pandas()['inactive'].to_list())*2\n",
    "labels = is_crack+is_inactive\n",
    "\n",
    "# perform stratified split of sample ids\n",
    "idx_train, idx_val = model_selection.train_test_split(list(range(len(all_data))), test_size=0.8, stratify=labels)\n",
    "\n",
    "\n",
    "# get subsets using sample ids\n",
    "train_data = all_data.pandas.iloc[idx_train]\n",
    "val_data = all_data.pandas.iloc[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate class weights\n",
    "\n",
    "The dataset is highly imbalanced. To this end, we compute weights for the two classes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(train_data)\n",
    "n_crack = len(train_data.pandas.query('crack == True'))\n",
    "n_inactive = len(train_data.pandas.query('inactive == True'))\n",
    "pos_weight = [n_crack / (n_samples-n_crack), n_inactive / (n_samples-n_inactive)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute statistics for normalization\n",
    "\n",
    "Here, we compute the mean and standard deviation using the first 50 images, since we do (not yet) support auto calibration, like you might know it from FastAI. However, we plan to [implement this](https://github.com/ma0ho/pvinspect/issues/6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(149.05248288888887, 43.36348877917837)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "mean = np.mean([x.data for x in train_data.pandas.iloc[:50]])\n",
    "std = np.std([x.data for x in train_data.pandas.iloc[:50]])\n",
    "(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up data augmentation pipeline\n",
    "\n",
    "This is as usual using PyTorch transforms. However, we support any data augmentation library that results in callable transform-objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = tv.transforms.Compose([\n",
    "    tv.transforms.Lambda(lambda x: x.astype(np.uint8)),   # this is required, since pvinspect converts all uint images to uint16 internally\n",
    "    tv.transforms.ToPILImage(),\n",
    "    tv.transforms.Resize((150, 150)),\n",
    "    tv.transforms.RandomVerticalFlip(),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.RandomAffine(degrees=10),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize([mean/255], [std/255]),  # statistics are computed on original images (uint8)\n",
    "    tv.transforms.Lambda(lambda x: x.repeat(3,1,1))\n",
    "])\n",
    "val_tfms = tv.transforms.Compose([\n",
    "    tv.transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    tv.transforms.ToPILImage(),\n",
    "    tv.transforms.Resize((150, 150)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize([mean/255], [std/255]),\n",
    "    tv.transforms.Lambda(lambda x: x.repeat(3,1,1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Convert `ImageSequence`s into `Dataset`s and create `DataLoader`s\n",
    "\n",
    "This is the main part of the PyTorch integration. Here, `pv.integration.pytorch.ClassificationDataset` extends `pv.integration.pytorch.Dataset`, which itself extends the PyTorch `Dataset`. Note that we are not restricted to classification tasks, since we can use the more general purpose `pv.integration.pytorch.Dataset` instead. However, `ClassificationDataset` conveniently converts meta attributes listet in the `meta_classes` attribute into one-hot tensors. Furthermore, it allows to convert classification results back into meta attributes of the `ImageSequence` using the [`result_sequence`](https://ma0ho.github.io/pvinspect/integration/pytorch/dataset.html#pvinspect.integration.pytorch.dataset.ClassificationDataset.result_sequence) method (not shown here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = pv.integration.pytorch.ClassificationDataset(train_data, meta_classes=['crack', 'inactive'], data_transform=train_tfms)\n",
    "val_ds = pv.integration.pytorch.ClassificationDataset(train_data, meta_classes=['crack', 'inactive'], data_transform=val_tfms)\n",
    "train_dl = t.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = t.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Set up `Trainer` and train model\n",
    "\n",
    "Now, set up a PyTorch Lightning `Trainer` and train the model using `train_dl` and `val_dl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DefectModel(pos_weight, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: True, used: True\nINFO:lightning:GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nINFO:lightning:TPU available: False, using: 0 TPU cores\nCUDA_VISIBLE_DEVICES: [0]\nINFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, logger=None, progress_bar_refresh_rate=20, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n  | Name  | Type   | Params\n---------------------------------\n0 | model | ResNet | 4 M   \nINFO:lightning:\n  | Name  | Type   | Params\n---------------------------------\n0 | model | ResNet | 4 M   \nEpoch 1:  34%|███▍      | 20/58 [00:03<00:06,  5.58it/s, loss=0.310]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 1:  69%|██████▉   | 40/58 [00:06<00:03,  5.72it/s, loss=0.191]\nEpoch 2:  34%|███▍      | 20/58 [00:02<00:04,  8.92it/s, loss=0.137]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 2:  69%|██████▉   | 40/58 [00:04<00:02,  8.93it/s, loss=0.124]\nEpoch 3:  34%|███▍      | 20/58 [00:02<00:03,  9.65it/s, loss=0.117]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 3:  69%|██████▉   | 40/58 [00:04<00:01,  9.97it/s, loss=0.111]\nEpoch 4:  34%|███▍      | 20/58 [00:01<00:03, 11.67it/s, loss=0.122]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 4:  69%|██████▉   | 40/58 [00:03<00:01, 10.99it/s, loss=0.109]\nEpoch 5:  34%|███▍      | 20/58 [00:01<00:03, 12.14it/s, loss=0.102]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 5:  69%|██████▉   | 40/58 [00:03<00:01, 11.32it/s, loss=0.108]\nEpoch 6:  34%|███▍      | 20/58 [00:01<00:03, 12.16it/s, loss=0.107]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 6:  69%|██████▉   | 40/58 [00:03<00:01, 10.59it/s, loss=0.105]\nEpoch 7:  34%|███▍      | 20/58 [00:01<00:03, 10.56it/s, loss=0.109]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 7:  69%|██████▉   | 40/58 [00:03<00:01, 10.55it/s, loss=0.105]\nEpoch 8:  34%|███▍      | 20/58 [00:01<00:03, 11.13it/s, loss=0.101]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 8:  69%|██████▉   | 40/58 [00:03<00:01, 10.29it/s, loss=0.096]\nEpoch 9:  34%|███▍      | 20/58 [00:02<00:04,  8.98it/s, loss=0.111]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 9:  69%|██████▉   | 40/58 [00:05<00:02,  7.57it/s, loss=0.097]\nEpoch 10:  34%|███▍      | 20/58 [00:02<00:04,  9.49it/s, loss=0.092]\nValidating: 0it [00:00, ?it/s]\u001b[A\nEpoch 10:  69%|██████▉   | 40/58 [00:04<00:02,  8.24it/s, loss=0.096]\nEpoch 10:  69%|██████▉   | 40/58 [00:04<00:02,  8.23it/s, loss=0.096]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[[1](http://dx.doi.org/10.1016/j.solener.2019.02.067)] Deitsch, S.; Christlein, V.; Berger, S.; Buerhop-Lutz, C.; Maier, A.; Gallwitz, F. & Riess, C. Automatic classification of defective photovoltaic module cells in electroluminescence images. Solar Energy, Elsevier BV, 2019, 185, 455-468."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}